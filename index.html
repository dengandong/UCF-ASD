
<!DOCTYPE html>
<html>
<!-- <head>
<meta charset="utf-8">
<title>菜鸟教程(runoob.com)</title>
</head>
<body>
 
<h1>我的第一个标题</h1>
 
<p>我的第一个段落。</p>
 
</body> -->
<!-- <!DOCTYPE html>
<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>SyncDreamer: Generating Multiview-consistent Images from a Single-view Image</title>
    <!-- Bootstrap -->
    <link href="css/bootstrap-4.4.1.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <!-- <link href='http://fonts.googleapis.com/css?family=Open+Sans:400italic,700italic,800italic,400,700,800' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="css/project.css" media="screen" />
    <link rel="stylesheet" type="text/css" media="screen" href="css/iconize.css" />
    <script src="js/google-code-prettify/prettify.js"></script> -->
  </head>

  <!-- cover -->
  <section>
    <div class="jumbotron text-center mt-0">
      <div class="container">
        <div class="row">
          <div class="col-12">
            <h2>Language-Assisted Deep Learning for Autistic Behaviors Recognition
            </h2>
            <h4 style="color:#5a6268;">Smart Health 2023 </h4>
            <hr>
            <h6>
                <a href="https://dengandong.github.io/" target="_blank">Andong Deng</a><sup>1</sup>,
                <a href="https://taoyang1122.github.io/" target="_blank">Taojiannan Yang</a><sup>1</sup>,
                <a href="https://www.crcv.ucf.edu/chenchen/index.html" target="_blank">Chen Chen</a><sup>1</sup>,
                <a href="https://klesse.utsa.edu/faculty/profiles/chen-qian.html" target="_blank">Qian Chen</a><sup>2</sup>,
                <a href="" target="_blank">Leslie Neely</a><sup>2</sup>,
                <a href="" target="_blank">Sakiko Oyama</a><sup>2</sup>,
                <a href="https://yuanli2333.github.io/" target="_blank">Li Yuan</a><sup>1,2</sup></h6>
            <p>  
                <sup>1</sup>University of Central Florida &nbsp;&nbsp;
                <sup>2</sup>University of Texas at San Antonio &nbsp;&nbsp;

            </p>

            <div class="row justify-content-center">
              <div class="column">
                  <p class="mb-5"><a class="btn btn-large btn-light" href="https://www.sciencedirect.com/science/article/pii/S2352648323000727" role="button"  target="_blank">
                    <i class="fa fa-file"></i> Paper</a> </p>
              </div>
              <!-- <div class="column">
                  <p class="mb-5"><a class="btn btn-large btn-light" href="https://github.com/PKU-YuanGroup/repaint123" role="button"  target="_blank">
                    <i class="fa fa-github-alt"></i> Code </a> </p>
              </div> -->
              <!-- <div class="column">
                  <p class="mb-5"><a class="btn btn-large btn-light" href="https://connecthkuhk-my.sharepoint.com/:f:/g/personal/yuanly_connect_hku_hk/EjYHbCBnV-VPjBqNHdNulIABq9sYAEpSz4NPLDI72a85vw" role="button"  target="_blank">
                    <i class="fa fa-database"></i> Model </a> </p>
              </div> -->
              <!-- <div class="column">
                  <p class="mb-5"><a class="btn btn-large btn-light" href="" role="button"  target="_blank">
                    <i class="fa fa-desktop"></i> Live Demo </a> </p>
              </div> -->
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>
 
   <!-- abstract -->
  <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
          <h3>Abstract</h3>
          <!-- <hr style="width:100%;height:0.7px;background-color:rgba(0, 0, 0, 0.665);margin-top:1em">
             Create high-fidelity 3D object mesh from a single image in <strong>5 SECONDS</strong>. -->
            <hr style="margin-top:0px">
            <!-- <h6 style="color:#8899a5"> Repaint123 crafts 3D content from a single image, matching 2D generation quality in just <strong>2 minutes</strong>. </h6> -->
            
            <img src="assets/grabs.png" alt="grab" width="90%">
            <br>
            <br>
            <!-- <h6 style="color:#8899a5"> SyncDreamer is able to directly generate multiview consistent images, which allows 3D reconstruction by NeuS or NeRF without SDS loss. </h6> -->

              <!-- <video width="80%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted="">
                  <source src="video/teaser.mp4" type="video/mp4">
              </video>

			        <br> -->
          
          <p class="text-left">
            Correctly recognizing the behaviors of children with Autism Spectrum Disorder (ASD) is of vital importance for the diagnosis of Autism and timely early intervention. However, the observation and recording during the treatment from the parents of autistic children may not be accurate and objective. In such cases, automatic recognition systems based on computer vision and machine learning (in particular deep learning) technology can alleviate this issue to a large extent. Existing human action recognition models can now achieve impressive performance on challenging activity datasets, e.g., daily activity, and sports activity. However, problem behaviors in children with ASD are very different from these general activities, and recognizing these problem behaviors via computer vision is less studied. In this paper, we first evaluate a strong baseline for action recognition, i.e., Video Swin Transformer, on two autism behaviors datasets (SSBD and ESBD) and show that it can achieve high accuracy and outperform the previous methods by a large margin, demonstrating the feasibility of vision-based problem behaviors recognition. Moreover, we propose language-assisted training to further enhance the action recognition performance. Specifically, we develop a two-branch multimodal deep learning framework by incorporating the ”freely available” language description for each type of problem behavior. Experimental results demonstrate that incorporating additional language supervision can bring an obvious performance boost for the autism problem behaviors recognition task as compared to using the video information only (i.e., 3.49% improvement on ESBD and 1.46% on SSBD). Our code and model will be publicly available for reproducing the results.          </p>
          <!-- </p>
          <img src="assets/architecture.png" alt="Overview" width="80%">
          <img src="assets/framework.png" alt="Overview" width="77%"> -->
          
          <!-- <video width="80%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted="">
            <source src="video/progress_low.mp4" type="video/mp4">
        </video> -->

        <p class="text-center">
          <!-- Reverse process of SyncDreamer's multiview diffusion. -->
          <!-- Overview of Our Repaint123. -->
        </p>
        </div>
      </div>
    </div>
  </section>
  <br>
  
   <!-- dataset -->
   <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
          <h3>Datasets and Pre-processing</h3>
            <hr style="margin-top:0px">
            <!-- <h6 style="color:#8899a5"> SyncDreamer is able to directly generate multiview consistent images, which allows 3D reconstruction by NeuS or NeRF without SDS loss. </h6> -->

              <!-- <video width="80%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted="">
                  <source src="video/teaser.mp4" type="video/mp4">
              </video>

			        <br> -->
      
          <!-- <h3>Image-to-3D generation pipeline</h3> -->

          <p class="text-left">
            There are two popular autism behavior datasets: Self-Stimulatory Behavior Dataset (SSBD) and Expanded Stereotype Behavior Dataset (ESBD). SSBD contains 75 videos of stimming actions of children with autism spectrum disorder with an average duration of 90 seconds per video. These videos were recorded in uncontrolled natural settings by parents/caregivers. SSBD contains three typical action classes: Arm Flapping, Head Banging, and Spinning. Arm Flapping is a stimming behavior when autistic children cannot normally communicate with people around them or they have excessive energy. Head banging is typically a self-injurious action where children will use their own hands to hit on the head or just directly hit other solid objects (e.g., desks and walls) with their heads. Spinning is also a stimming behavior that the children often turn around their whole body repeatedly. Each category includes 25 video samples, but some video URLs are no longer available, and we only obtain 59 videos: 19 for Arm flapping and 20 for both Head banging and Spinning. The second dataset ESBD contains 99 YouTube videos, and the average video duration is about 2 minutes. Besides the three categories of SSBD, ESBD also includes the fourth category of Hand Action; thus ESBD contains  35 videos for Arm flapping, 13 videos for Hand action, 24 videos for Head banging, and 37 videos for Spinning. Note that hand action and arm flapping are different, hand action focuses more on the finger actions and can describe more actions than arm flapping. Similar to SSBD, due to poor maintenance, we only obtained 89 videos of ESBD, namely 29 videos for Arm flapping, 13 videos for Hand action, 16 videos for Head banging, and 31 videos for Spinning. 
          </p>
          <img src="assets/samples.png" alt="samples" width="80%">
          <br>

          <!-- <p class="text-left">
          <h3>Controllable repainting scheme</h3> -->
        </p>
            <!-- <hr style="margin-top:0px"> -->
          <p class="text-left">
            These two datasets are noisy and contain a large portion of the background or other subjects. To enhance recognition accuracy, we first preprocess the videos to obtain cleaner data that include target children performing ASD behaviors only. To this end, we leverage one of the most popular object detection models YOLOv5, which is pretrained on the COCO dataset, as our target child detector (i.e., person detector). With the help of YOLOv5, we obtain the cropped frames of the target children and behaviors. As presented in Figure~\ref{preprocess}, the cropped frame is much more focused on the child region than the raw data, which leads to a more stable training process and better action recognition results. In addition, we segment each video into several 30-frame clips to extract as much information as possible, and this video segmentation setting also increases the number of training samples. Our final training data, therefore, contains 1,437 and 1,030 clips for ESBD and SSBD, respectively.
          </p>
          <img src="assets/preprocessing.png" alt="preprocess" width="77%">
          
          <!-- <video width="80%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted="">
            <source src="video/progress_low.mp4" type="video/mp4">
        </video> -->

        <p class="text-center">
          <!-- Reverse process of SyncDreamer's multiview diffusion. -->
          <!-- Overview of Our Repaint123. -->
        </p>
        </div>
      </div>
    </div>
  </section>
  <br>

   <!-- <Method> -->
   <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
          <h3>Method</h3>
          <!-- <hr style="width:100%;height:0.7px;background-color:rgba(0, 0, 0, 0.665);margin-top:1em">
             Create high-fidelity 3D object mesh from a single image in <strong>5 SECONDS</strong>. -->
            <hr style="margin-top:0px">
            <!-- <h6 style="color:#8899a5"> Repaint123 crafts 3D content from a single image, matching 2D generation quality in just <strong>2 minutes</strong>. </h6> -->

            <br>
          
          <p class="text-left">
            We propose to use detailed action descriptions as the textual input. Specifically, we first pre-define textural descriptions for the problem behavior classes by searching on the web and publications, as listed below. During training, when the visual branch takes as input a specific category of video, the corresponding action category description will be first tokenized into a language input tensor, and then the text encoder will encode the language input to a language feature.
          </p>  
          <img src="assets/text_tab.png" alt="text" width="90%">
          <br>

          <p class="text-left">
            In this work,  we utilize the CLIP text encoder as our text encoder. The original CLIP model consists of an image Transformer encoder and a text Transformer encoder and is trained on a large-scale image-text pairs dataset. The supervision contained in the natural language provides strong cross-modal representation ability for both CLIP encoders. In our work, in order to introduce additional supervision without bringing in extra annotations, we utilize the abundant image-text knowledge in the CLIP text encoder. As shown below, since the language feature from the text encoder actually contains sufficient corresponding visual information, we propose to minimize the distance between the visual feature from the Video Swin Transformer and the language feature in the feature space for the input visual and text pair. With such a strategy, the additional language knowledge is actually distilled into the visual branch, introducing more information for the recognition task and making the language feature more predictable.          
          </p>  
          <img src="assets/framework.png" alt="framework" width="90%">
          <br>

        <p class="text-center">
          <!-- Reverse process of SyncDreamer's multiview diffusion. -->
          <!-- Overview of Our Repaint123. -->
        </p>
        </div>
      </div>
    </div>
  </section>
  <br>

  <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
            <h3>Method</h3>
            <hr style="margin-top:0px">
            <!-- <p>Repaint123 generates high-quality and view-consistent 3D objects from a single unposed image. </p> -->

            <!-- <hr style="margin-top:0px">
            <video width="80%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted="">
                <source src="video/gso.mp4" type="video/mp4">
            </video>
          <p class="text-left"> -->
            
          </p>
        </div>
      </div>
      <div class="container align-items-center">
        <div class="row">
            <!-- Row 0 -->
            <div class="col-sm-1 d-flex align-items-center justify-content-start p-0">
                <div class="method-title">
                    Input
                </div>
            </div>
            <div class="col-sm-2">
                <img src="assets/videos/reference/cherry_1.png" alt="cherries" class="img-fluid " >
            </div>
            <div class="col-sm-2">
                <img src="assets/videos/reference/horse_watercolor_2.png" alt="donuts" class="img-fluid " >
            </div>
            <div class="col-sm-2">
                <img src="assets/videos/reference/hum.png" alt="hamburger" class="img-fluid "  >
            </div>
            <div class="col-sm-2">
                <img src="assets/videos/reference/horse.png" alt="horse" class="img-fluid " >
            </div>
            <div class="col-sm-2">
              <img src="assets/videos/reference/dragon_statue_2.png" alt="stone dragon statue" class="img-fluid " >
          </div>
          <!-- <div class="col-sm-2">
              <img src="assets/videos/reference/horse_watercolor_2.png" alt="watercolor horse" class="img-fluid " >
          </div> -->
        </div>
        


  <!-- citing -->
  <div class="container">
    <div class="row ">
      <div class="col-12">
          <h3>Citation</h3>
          <hr style="margin-top:0px">
              <pre style="background-color: #e9eeef;padding: 1.25em 1.5em">
<code> @misc{zhang2023repaint123,
      title={Repaint123: Fast and High-quality One Image to 3D Generation with Progressive Controllable 2D Repainting},
      author={Junwu Zhang and Zhenyu Tang and Yatian Pang and Xinhua Cheng and Peng Jin and Yida Wei and Wangbo Yu and Munan Ning and Li Yuan},
      year={2023},
      eprint={2312.13271},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}</code>
              </pre>
          <hr>
      </div>
    </div>
  </div>

  <footer class="text-center" style="margin-bottom:10px">
      Thanks to <a href="https://lioryariv.github.io/" target="_blank">Lior Yariv</a> for the website template.
  </footer>

</body>
</html>
